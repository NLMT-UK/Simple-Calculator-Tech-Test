# Simple Calculator Automated Test Framework

## Overview

This project implements a data-driven automated test framework using Python, Pytest, Selenium, and Allure.
The application under test is the **Simple Calculator**, which intentionally contains multiple builds with known defects to simulate real-world testing challenges.

The framework focuses on:

- Page Object Model (POM) design  
- Data-driven testing for arithmetic and validation scenarios  
- Cross-build regression coverage  
- Defect documentation via Allure with screenshots and HTML evidence  
- Scalability and CI/CD readiness  

This repository excludes virtual environments and generated Allure reports via `.gitignore`.
        After cloning, create and activate a virtual environment, then install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Project Structure

```
        Simple-Calculator-Tech-Test/
│
├── src/
│   ├── pages/
│   │   └── calculator_page.py          → Page Object Model for calculator UI
│   └── utils/
│       ├── web.py                      → WebDriver setup and configuration
│       └── waits.py                    → Explicit wait utilities
│
├── tests/
│   ├── test_data_driven_arithmetic.py  → Data-driven arithmetic regression across all builds
│   ├── test_edge_cases_and_validation.py → Validation and edge-case tests driven by CSV
│   ├── test_build_regression.py        → Build-focused checks to surface defects across builds
│   └── test_smoke_calculator.py        → Lightweight smoke/sanity checks for the calculator
│
├── data/
│   ├── arithmetic_cases.json           → Input data sets for arithmetic operations (used by regression tests)
│   └── edge_cases.csv                  → Edge/validation scenarios (used by validation tests)
│
├── reports/
│   ├── allure-results/                 → Raw Allure result files generated by pytest
│   ├── allure-report-[timestamp]/      → Generated Allure HTML reports (timestamped)
│   └── screenshots/                    → Screenshots captured for failed, xfailed, or xpassed tests
│
├── .github/
│   └── workflows/                      → CI/CD pipeline definitions for GitHub Actions
│
├── conftest.py                         → Shared pytest fixtures and Allure hooks
├── pytest.ini                          → Pytest configuration and custom markers
├── pyproject.toml                      → Tooling configuration (formatters, linters, pytest settings)
├── requirements.txt                    → Project dependencies
├── run_all_tests.sh                    → Shell script to run all tests and generate reports
├── run_all_tests.py                    → Python script for running the entire suite on any platform
└── README.md                           → Framework documentation (this file)
```

## Running the Full Test Suite

Two scripts are provided for running the full regression suite end-to-end:

- `run_all_tests.sh` – for macOS/Linux  
- `run_all_tests.py` – cross-platform (works on Windows)

Both scripts:

1. Clean old Allure result files.
2. Run the entire pytest suite using two parallel workers (`-n 2`) for stability and speed.
3. Capture all failures, xfails, and skipped tests without stopping on the first failure.
4. Generate a timestamped Allure HTML report under `reports/allure-report-[timestamp]/`.
5. Open the Allure report automatically in the default browser.

These scripts ensure consistent and reproducible results with complete reporting and evidence capture.

### Option 1: Using the shell script (macOS/Linux)

```bash
chmod +x run_all_tests.sh
./run_all_tests.sh
```

### Option 2: Using the Python script (cross-platform)

```bash
python run_all_tests.py
```

This is useful on Windows or in CI environments where shell execution may not be supported.

### Virtual environment and dependencies

Both scripts attempt to activate a `.venv` directory (if present) and install dependencies from `requirements.txt`.
        If you prefer to manage environments manually:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

This ensures the test environment is reproducible on any system.

## Framework Design and Architecture

### Page Object Model (POM)

        All UI interactions are encapsulated within `src/pages/calculator_page.py`.

The `CalculatorPage` class exposes methods such as:

- `open(base_url)`  
- `choose_build(build)`  
- `set_numbers(a, b)`  
- `choose_operation(op)`  
- `set_integer_only(enabled)`  
- `calculate()`  
- `read_answer()`

This design isolates element locators and interaction logic from the tests themselves, making the framework more maintainable.

### Data-Driven Testing

        Arithmetic test data is stored in `data/arithmetic_cases.json`, and validation cases in `data/edge_cases.csv`.
Pytest dynamically parameterises tests from these files across all calculator builds (0–9).

This approach allows:

- Expanding coverage without modifying test logic  
- Parallel execution for efficiency  
- Clear, traceable data-to-test relationships  

## Test Types and Coverage

| Test Type               | Description                                                               | Location                                |
|-------------------------|---------------------------------------------------------------------------|-----------------------------------------|
| Regression (Arithmetic) | Verifies all operations across builds using multiple numeric combinations | tests/test_data_driven_arithmetic.py    |
| Validation (Edge Cases) | Handles divide-by-zero, invalid input, and large number cases             | tests/test_edge_cases_and_validation.py |
| Smoke                   | Basic functional checks for working builds                                | tests/test_smoke_calculator.py          |

## Defect Handling and Reporting Strategy

The Simple Calculator application explicitly notes that builds 1–8 contain known defects, and exploratory testing confirmed that build 9 is missing key elements (such as the Calculate button and second number input).

These known issues are handled with **pytest xfail markers** to prevent false pipeline failures while still capturing evidence.

- Xfail marks known defects without blocking CI/CD runs.  
- Screenshots and HTML sources are attached automatically for traceability.  
- Allure reports clearly indicate failed or expected-fail tests.

        In a production context:

1. Initial failures would be logged as defects (e.g. Jira).
2. Once accepted, tests would be marked xfail until fixed.
3. When resolved, the xfail would be removed to verify the fix.

### Allure Reporting

        Each test automatically attaches:

- A screenshot of the browser  
- The full HTML page source  

        Attachments appear in Allure for all failed, xfailed, or xpassed tests.

### Generating and Viewing Reports

Generate results:

```bash
pytest -m regression --alluredir=reports/allure-results
```

Generate an HTML report:

```bash
        allure generate reports/allure-results -o reports/allure-report --clean
```

Optionally generate timestamped reports:

```bash
        allure generate reports/allure-results -o reports/allure-report-2025-11-04_14-23 --clean
```

Open the report:

```bash
        allure open reports/allure-report
```

The Allure overview includes:

- Pass/fail/skip/xfail statistics  
- Screenshots and HTML evidence per test  
- Environment metadata (browser, OS, framework)  
- Historical trends (when integrated with CI/CD history)

The Trend and Executors sections remain blank for local runs because they require persisted CI metadata.

## Key Fixtures (conftest.py)

| Fixture                   | Scope    | Purpose                                                   |
|---------------------------|----------|-----------------------------------------------------------|
| driver                    | session  | Initialises headless Chrome WebDriver                     |
| calc                      | function | Returns CalculatorPage instance for the current test      |
| write_allure_environment  | session  | Writes environment metadata for Allure reports            |
| pytest_runtest_makereport | hook     | Captures screenshots and HTML for failed or xfailed tests |

## pyproject.toml

`pyproject.toml` provides configuration for tools such as:

- Code formatters (Black)  
- Linters (Flake8 or Ruff)  
- Import sorters (isort)  
- Pytest defaults and coverage settings  

        Including this file helps standardise the environment and ensures future scalability.

## CI/CD Integration

The `.github/workflows/` directory contains a GitHub Actions workflow to execute tests automatically on push or pull requests.

    Example workflow snippet:

```yaml
name: CI

on:
push:
branches: [ main ]
pull_request:

jobs:
tests:
runs-on: ubuntu-latest

steps:
      - name: Checkout repository
uses: actions/checkout@v4

      - name: Set up Python
uses: actions/setup-python@v5
with:
python-version: '3.12'

      - name: Install dependencies
run: pip install -r requirements.txt

      - name: Run tests
run: pytest -n 2 --maxfail=0 --alluredir=reports/allure-results

      - name: Generate Allure report
run: allure generate reports/allure-results -o reports/allure-report --clean

      - name: Upload Allure report as artifact
uses: actions/upload-artifact@v4
with:
name: allure-report
path: reports/allure-report
```

This workflow installs dependencies, runs all tests, generates an Allure report, and uploads the output as an artifact.

## Scalability and Future Enhancements

| Category                   | Enhancement                                     | Benefit                                             |
|----------------------------|-------------------------------------------------|-----------------------------------------------------|
| Browser Matrix             | Add support for Edge, Firefox, Safari           | Cross-browser confidence                            |
| Dockerisation              | Containerise framework and browser              | Consistent environments                             |
| Cloud Execution            | Integration with Selenium Grid or BrowserStack  | Scalable distributed testing                        |
| Jira API Integration       | Automatically log failed/xfail tests as tickets | Streamlined defect tracking                         |
| Test Data Expansion        | Extend JSON and CSV datasets                    | Greater coverage                                    |
| Allure TestOps Integration | Centralise results and analytics                | Better historical insights                          |
| Test Design                | Add BDD (pytest-bdd or behave) layer            | Improved readability for non-technical stakeholders |

## Maintenance Guidelines

- Add new UI locators or interactions only in `calculator_page.py`.  
- Extend test data in `data/` rather than editing test logic.  
- Clean `reports/allure-results` and `reports/screenshots` periodically.  
- Keep local and CI environments aligned to avoid flakiness.  
- Group future tests by purpose using clear pytest markers.

## BDD

This project does not include BDD-style tests.
        Given the calculator’s simple design and focus on data variation rather than business workflow, a data-driven approach is more efficient.

        If stakeholder-readable tests become a requirement, BDD tools such as `pytest-bdd` or `behave` can be integrated easily — the existing Page Object structure already supports this.

## Summary

This framework demonstrates:

- A modular Page Object architecture  
- Strong data-driven testing principles  
- Evidence-based reporting with Allure  
- CI/CD integration and maintainability  
- Clear defect tracking via xfail and visual artefacts  

        It provides a solid foundation for scalable regression automation and transparent quality reporting.

---

**Author:** Nick Taylor
**Date:** November 2025
**Tools:** Python, Pytest, Selenium, Allure
**Website under test:** https://testsheepnz.github.io/BasicCalculator.html
